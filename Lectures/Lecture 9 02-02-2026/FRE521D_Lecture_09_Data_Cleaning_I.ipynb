{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lecture 9: Data Cleaning I - Formats, Types, Keys, and Deduplication\n",
    "\n",
    "**Date:** Monday, February 2, 2026  \n",
    "**Instructor:** Asif Ahmed Neloy  \n",
    "**Program:** UBC Master of Food and Resource Economics\n",
    "\n",
    "---\n",
    "\n",
    "### Today's Agenda\n",
    "\n",
    "1. Understanding Dirty Data\n",
    "2. Data Type Standardization\n",
    "3. Handling European Number Formats\n",
    "4. Date and Time Parsing\n",
    "5. String Cleaning and Normalization\n",
    "6. Key Generation for Unique Identifiers\n",
    "7. Deduplication Strategies\n",
    "8. Building Reusable Cleaning Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Identify common data quality issues in real-world datasets\n",
    "2. Convert European number formats (comma decimals) to standard floats\n",
    "3. Parse dates and times from various string formats\n",
    "4. Clean and normalize string data\n",
    "5. Generate unique keys for records that lack them\n",
    "6. Detect and remove duplicate records using multiple strategies\n",
    "7. Build reusable cleaning functions for production pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 60)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Ready for Data Cleaning I!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports above include `re` for regular expressions (pattern matching in strings) and `hashlib` for generating hash-based unique keys. These are essential tools for data cleaning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Dirty Data\n",
    "\n",
    "### What is \"Dirty\" Data?\n",
    "\n",
    "Dirty data is data that contains errors, inconsistencies, or is otherwise unsuitable for analysis. Studies show that data scientists spend 60-80% of their time cleaning data.\n",
    "\n",
    "### Common Data Quality Issues\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   TYPES OF DIRTY DATA                          │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  STRUCTURAL ISSUES          CONTENT ISSUES                     │\n",
    "│  ├── Wrong data types       ├── Missing values                 │\n",
    "│  ├── Inconsistent formats   ├── Outliers                       │\n",
    "│  ├── Mixed encodings        ├── Typos and misspellings         │\n",
    "│  └── Nested structures      └── Invalid values                 │\n",
    "│                                                                 │\n",
    "│  CONSISTENCY ISSUES         COMPLETENESS ISSUES                │\n",
    "│  ├── Duplicates             ├── Missing records                │\n",
    "│  ├── Contradictions         ├── Truncated data                 │\n",
    "│  ├── Referential breaks     ├── Partial records                │\n",
    "│  └── Unit mismatches        └── Missing columns                │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### The Cost of Dirty Data\n",
    "\n",
    "Dirty data leads to:\n",
    "- **Wrong conclusions**: Flawed analysis from flawed inputs\n",
    "- **Failed pipelines**: ETL jobs crash on unexpected values\n",
    "- **Wasted time**: Hours debugging issues that stem from data quality\n",
    "- **Lost trust**: Stakeholders question results when errors surface\n",
    "\n",
    "### Our Datasets\n",
    "\n",
    "Today we will work with two datasets that have real-world quality issues:\n",
    "\n",
    "1. **AirQualityUCI.csv**: European dataset with comma decimals and coded missing values\n",
    "2. **GlobalWeatherRepository.csv**: Large dataset with potential duplicates and inconsistencies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AirQuality dataset\n",
    "# Note: This file uses semicolons as delimiters (common in European CSVs)\n",
    "\n",
    "air_quality_path = '../../Datasets/AirQualityUCI.csv'\n",
    "\n",
    "# First, let's look at the raw file to understand its structure\n",
    "print(\"First 3 lines of AirQualityUCI.csv:\")\n",
    "print(\"-\" * 80)\n",
    "with open(air_quality_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(line.strip()[:100] + \"...\")\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice several issues in the raw data:\n",
    "1. **Semicolon delimiter** instead of comma (European standard)\n",
    "2. **Comma as decimal separator** (e.g., \"2,6\" instead of \"2.6\")\n",
    "3. **Extra empty columns** at the end\n",
    "\n",
    "These are classic signs of a European-formatted CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with semicolon delimiter\n",
    "df_air = pd.read_csv(air_quality_path, sep=';', encoding='utf-8')\n",
    "\n",
    "print(f\"Shape: {df_air.shape}\")\n",
    "print(f\"\\nColumns ({len(df_air.columns)}):\")\n",
    "for i, col in enumerate(df_air.columns):\n",
    "    print(f\"  {i+1:2d}. '{col}'\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_air.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded, but there are issues to fix:\n",
    "- Extra unnamed columns at the end\n",
    "- Numeric columns are stored as strings because of comma decimals\n",
    "- The value -200 appears frequently (this is the missing value code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data types\n",
    "print(\"Data Types:\")\n",
    "print(df_air.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns that should be numeric (like CO(GT), C6H6(GT)) are showing as `object` type, which means they contain strings. This is because pandas cannot automatically parse European number formats.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Type Standardization\n",
    "\n",
    "### Why Types Matter\n",
    "\n",
    "Wrong data types cause problems:\n",
    "\n",
    "| Issue | Example | Consequence |\n",
    "|-------|---------|-------------|\n",
    "| Math fails | `\"2.5\" + \"3.0\"` | Returns `\"2.53.0\"` (string concat) |\n",
    "| Sorting breaks | `[\"10\", \"2\", \"1\"]` | Sorts as `[\"1\", \"10\", \"2\"]` |\n",
    "| Aggregations wrong | `mean(\"2\", \"3\")` | Error or NaN |\n",
    "| Memory waste | Strings use 10x more memory than numbers |\n",
    "\n",
    "### Type Conversion Strategy\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                TYPE CONVERSION WORKFLOW                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   1. IDENTIFY  ───►  Which columns need conversion?             │\n",
    "│                      Look at dtypes and sample values           │\n",
    "│                                                                 │\n",
    "│   2. CLEAN     ───►  Remove/replace characters that block       │\n",
    "│                      conversion (commas, currency symbols)      │\n",
    "│                                                                 │\n",
    "│   3. CONVERT   ───►  Apply type conversion with error handling  │\n",
    "│                      Use pd.to_numeric() or astype()            │\n",
    "│                                                                 │\n",
    "│   4. VERIFY    ───►  Check results and handle failures          │\n",
    "│                      Look for NaN from failed conversions       │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's drop the empty columns at the end\n",
    "# These are artifacts from the CSV export\n",
    "\n",
    "# Find columns that are entirely empty\n",
    "empty_cols = [col for col in df_air.columns if df_air[col].isna().all()]\n",
    "print(f\"Empty columns to drop: {empty_cols}\")\n",
    "\n",
    "# Also drop unnamed columns\n",
    "unnamed_cols = [col for col in df_air.columns if 'Unnamed' in str(col)]\n",
    "print(f\"Unnamed columns to drop: {unnamed_cols}\")\n",
    "\n",
    "# Drop both\n",
    "cols_to_drop = list(set(empty_cols + unnamed_cols))\n",
    "df_air_clean = df_air.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\nShape after dropping: {df_air_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the empty columns that were artifacts of the European CSV export. Now we have a cleaner dataset to work with.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling European Number Formats\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In many European countries, the decimal separator is a comma, not a period:\n",
    "\n",
    "| Region | Number Format | Example |\n",
    "|--------|--------------|----------|\n",
    "| US/UK | Period decimal | 1,234.56 |\n",
    "| Europe (many) | Comma decimal | 1.234,56 |\n",
    "\n",
    "When data is exported from European systems, pandas sees \"2,6\" and treats it as a string because it doesn't recognize the comma as a decimal point.\n",
    "\n",
    "### Solution: Replace and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_european_decimal(series):\n",
    "    \"\"\"\n",
    "    Convert a series with European decimal format (comma) to numeric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Series with European-formatted numbers (e.g., \"2,6\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series : Numeric series with proper float values\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> s = pd.Series(['2,6', '3,14', '-1,5'])\n",
    "    >>> convert_european_decimal(s)\n",
    "    0    2.60\n",
    "    1    3.14\n",
    "    2   -1.50\n",
    "    \"\"\"\n",
    "    # Step 1: Convert to string (handles mixed types)\n",
    "    str_series = series.astype(str)\n",
    "    \n",
    "    # Step 2: Replace comma with period\n",
    "    str_series = str_series.str.replace(',', '.', regex=False)\n",
    "    \n",
    "    # Step 3: Convert to numeric, coercing errors to NaN\n",
    "    numeric_series = pd.to_numeric(str_series, errors='coerce')\n",
    "    \n",
    "    return numeric_series\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_values = pd.Series(['2,6', '3,14', '-1,5', 'N/A', '100'])\n",
    "print(\"Before conversion:\")\n",
    "print(test_values)\n",
    "print(f\"Type: {test_values.dtype}\")\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "converted = convert_european_decimal(test_values)\n",
    "print(converted)\n",
    "print(f\"Type: {converted.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `convert_european_decimal` function handles the conversion in three steps:\n",
    "\n",
    "1. **Convert to string**: Ensures consistent input type\n",
    "2. **Replace comma with period**: Transforms European to US format\n",
    "3. **Convert to numeric**: Uses `pd.to_numeric` with error handling\n",
    "\n",
    "The `errors='coerce'` parameter is important: it converts invalid values to NaN instead of raising an error, which allows the pipeline to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that need European decimal conversion\n",
    "# These are columns with object dtype that should be numeric\n",
    "\n",
    "# Check which columns have comma in their values\n",
    "def check_european_format(df):\n",
    "    \"\"\"\n",
    "    Identify columns that appear to have European decimal format.\n",
    "    \n",
    "    Returns a list of column names where values contain commas\n",
    "    that look like decimal separators.\n",
    "    \"\"\"\n",
    "    european_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Sample non-null values\n",
    "            sample = df[col].dropna().head(100).astype(str)\n",
    "            \n",
    "            # Check if values match pattern like \"2,6\" or \"-1,23\"\n",
    "            comma_count = sample.str.contains(r'^-?\\d+,\\d+$', regex=True).sum()\n",
    "            \n",
    "            if comma_count > 10:  # If more than 10% have this pattern\n",
    "                european_cols.append(col)\n",
    "    \n",
    "    return european_cols\n",
    "\n",
    "\n",
    "european_columns = check_european_format(df_air_clean)\n",
    "print(f\"Columns with European decimal format ({len(european_columns)}):\")\n",
    "for col in european_columns:\n",
    "    sample_val = df_air_clean[col].dropna().iloc[0] if df_air_clean[col].notna().any() else 'N/A'\n",
    "    print(f\"  - {col}: example value = '{sample_val}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've identified which columns need conversion. The detection function looks for the pattern of digits, comma, digits which is characteristic of European decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all European decimal columns\n",
    "print(\"Converting European decimal columns...\\n\")\n",
    "\n",
    "for col in european_columns:\n",
    "    original_dtype = df_air_clean[col].dtype\n",
    "    original_sample = df_air_clean[col].iloc[0]\n",
    "    \n",
    "    df_air_clean[col] = convert_european_decimal(df_air_clean[col])\n",
    "    \n",
    "    new_dtype = df_air_clean[col].dtype\n",
    "    new_sample = df_air_clean[col].iloc[0]\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Before: {original_sample} ({original_dtype})\")\n",
    "    print(f\"  After:  {new_sample} ({new_dtype})\")\n",
    "\n",
    "print(\"\\nConversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the European decimal columns have been converted to proper float types. The example values show the transformation: \"2,6\" became 2.6.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Date and Time Parsing\n",
    "\n",
    "### The Challenge with Dates\n",
    "\n",
    "Date formats vary widely across sources:\n",
    "\n",
    "| Format | Example | Region/Standard |\n",
    "|--------|---------|------------------|\n",
    "| DD/MM/YYYY | 25/12/2025 | Europe, most of world |\n",
    "| MM/DD/YYYY | 12/25/2025 | United States |\n",
    "| YYYY-MM-DD | 2025-12-25 | ISO 8601 (databases) |\n",
    "| DD-Mon-YY | 25-Dec-25 | Legacy systems |\n",
    "\n",
    "### Ambiguous Dates\n",
    "\n",
    "The date \"01/02/2025\" could mean:\n",
    "- January 2, 2025 (US format)\n",
    "- February 1, 2025 (European format)\n",
    "\n",
    "You must know your data source to parse correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the date column in our air quality data\n",
    "print(\"Date column sample:\")\n",
    "print(df_air_clean['Date'].head(10))\n",
    "print(f\"\\nCurrent dtype: {df_air_clean['Date'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date format is DD/MM/YYYY (European format). We need to specify this when parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_smart(series, formats=None):\n",
    "    \"\"\"\n",
    "    Parse date strings by trying multiple formats.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Series with date strings\n",
    "    formats : list\n",
    "        List of date formats to try, in order of preference\n",
    "        Default: common European and US formats\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series : Datetime series\n",
    "    \"\"\"\n",
    "    if formats is None:\n",
    "        formats = [\n",
    "            '%d/%m/%Y',    # European: 25/12/2025\n",
    "            '%Y-%m-%d',    # ISO: 2025-12-25\n",
    "            '%m/%d/%Y',    # US: 12/25/2025\n",
    "            '%d-%m-%Y',    # European with dashes\n",
    "            '%Y/%m/%d',    # Alternative ISO\n",
    "        ]\n",
    "    \n",
    "    # Try pandas' automatic parsing first\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            result = pd.to_datetime(series, format=fmt, errors='coerce')\n",
    "            \n",
    "            # Check if most values parsed successfully\n",
    "            success_rate = result.notna().sum() / len(result)\n",
    "            \n",
    "            if success_rate > 0.9:  # 90% success threshold\n",
    "                print(f\"  Parsed with format '{fmt}' (success rate: {success_rate:.1%})\")\n",
    "                return result\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Fallback to pandas inference\n",
    "    print(\"  Using pandas automatic inference\")\n",
    "    return pd.to_datetime(series, errors='coerce', dayfirst=True)\n",
    "\n",
    "\n",
    "# Parse the Date column\n",
    "print(\"Parsing Date column:\")\n",
    "df_air_clean['Date'] = parse_date_smart(df_air_clean['Date'])\n",
    "\n",
    "print(f\"\\nNew dtype: {df_air_clean['Date'].dtype}\")\n",
    "print(f\"Date range: {df_air_clean['Date'].min()} to {df_air_clean['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parse_date_smart` function tries multiple date formats and uses the one with the highest success rate. This is more robust than assuming a single format.\n",
    "\n",
    "Key features:\n",
    "1. **Multiple format attempts**: Tries common formats in order of likelihood\n",
    "2. **Success rate check**: Ensures the format actually works for most data\n",
    "3. **Fallback option**: Uses pandas inference if explicit formats fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the Time column\n",
    "# Time is in format HH.MM.SS (dots instead of colons - European style)\n",
    "\n",
    "print(\"Time column sample:\")\n",
    "print(df_air_clean['Time'].head())\n",
    "\n",
    "# Replace dots with colons for standard time format\n",
    "df_air_clean['Time'] = df_air_clean['Time'].astype(str).str.replace('.', ':', regex=False)\n",
    "\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(df_air_clean['Time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time column used periods as separators (18.00.00) instead of colons (18:00:00). A simple string replacement standardizes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Date and Time into a single datetime column\n",
    "\n",
    "df_air_clean['datetime'] = pd.to_datetime(\n",
    "    df_air_clean['Date'].astype(str) + ' ' + df_air_clean['Time'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "print(\"Combined datetime column:\")\n",
    "print(df_air_clean[['Date', 'Time', 'datetime']].head())\n",
    "\n",
    "# Now we can do time-based analysis\n",
    "print(f\"\\nDatetime range:\")\n",
    "print(f\"  Start: {df_air_clean['datetime'].min()}\")\n",
    "print(f\"  End:   {df_air_clean['datetime'].max()}\")\n",
    "print(f\"  Duration: {df_air_clean['datetime'].max() - df_air_clean['datetime'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a proper datetime column enables:\n",
    "- Time-series analysis\n",
    "- Grouping by hour, day, month\n",
    "- Filtering by date ranges\n",
    "- Calculating time differences\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. String Cleaning and Normalization\n",
    "\n",
    "### Common String Problems\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                   STRING QUALITY ISSUES                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  WHITESPACE           CASE                    ENCODING          │\n",
    "│  \" Canada \"           \"CANADA\"                \"Caf\\xe9\"         │\n",
    "│  \"Canada  \"           \"canada\"                \"Cafu00e9\"        │\n",
    "│  \"  Canada\"           \"Canada\"                                  │\n",
    "│                       \"CaNaDa\"                                  │\n",
    "│                                                                 │\n",
    "│  SPECIAL CHARS        VARIATIONS             ABBREVIATIONS     │\n",
    "│  \"Canada!\"            \"United States\"        \"US\" vs \"USA\"     │\n",
    "│  \"Canada\\n\"           \"United States of...\"  \"UK\" vs \"U.K.\"    │\n",
    "│  \"Canada\\t\"           \"U.S.A.\"                                  │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Normalization Strategy\n",
    "\n",
    "Normalization means transforming strings to a consistent format:\n",
    "1. Strip leading/trailing whitespace\n",
    "2. Normalize internal whitespace (multiple spaces to one)\n",
    "3. Standardize case (usually lowercase or title case)\n",
    "4. Remove or replace special characters\n",
    "5. Apply domain-specific mappings (e.g., country name variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(series, case='lower', strip=True, collapse_whitespace=True):\n",
    "    \"\"\"\n",
    "    Normalize string values in a series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Series with string values\n",
    "    case : str\n",
    "        'lower', 'upper', 'title', or None for no change\n",
    "    strip : bool\n",
    "        Remove leading/trailing whitespace\n",
    "    collapse_whitespace : bool\n",
    "        Replace multiple spaces with single space\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series : Normalized string series\n",
    "    \"\"\"\n",
    "    # Convert to string type\n",
    "    result = series.astype(str)\n",
    "    \n",
    "    # Strip whitespace\n",
    "    if strip:\n",
    "        result = result.str.strip()\n",
    "    \n",
    "    # Collapse multiple whitespace\n",
    "    if collapse_whitespace:\n",
    "        result = result.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Apply case transformation\n",
    "    if case == 'lower':\n",
    "        result = result.str.lower()\n",
    "    elif case == 'upper':\n",
    "        result = result.str.upper()\n",
    "    elif case == 'title':\n",
    "        result = result.str.title()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Demonstrate with messy data\n",
    "messy_data = pd.Series([\n",
    "    '  Canada  ',\n",
    "    'UNITED STATES',\n",
    "    'united   kingdom',\n",
    "    'Brazil',\n",
    "    '   Germany\\t',\n",
    "])\n",
    "\n",
    "print(\"Original (showing repr to see whitespace):\")\n",
    "for val in messy_data:\n",
    "    print(f\"  '{val}'\")\n",
    "\n",
    "print(\"\\nNormalized (lowercase):\")\n",
    "normalized = normalize_string(messy_data, case='lower')\n",
    "for val in normalized:\n",
    "    print(f\"  '{val}'\")\n",
    "\n",
    "print(\"\\nNormalized (title case):\")\n",
    "normalized_title = normalize_string(messy_data, case='title')\n",
    "for val in normalized_title:\n",
    "    print(f\"  '{val}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `normalize_string` function handles the most common string cleaning tasks:\n",
    "\n",
    "1. **strip()**: Removes leading and trailing whitespace\n",
    "2. **Regex replace**: `\\s+` matches one or more whitespace characters, replaced with single space\n",
    "3. **Case transformation**: Standardizes capitalization\n",
    "\n",
    "After normalization, \"  Canada  \" and \"CANADA\" both become \"canada\" (or \"Canada\" in title case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather data to demonstrate country name normalization\n",
    "weather_path = '../../Datasets/GlobalWeatherRepository.csv'\n",
    "df_weather = pd.read_csv(weather_path)\n",
    "\n",
    "print(f\"Weather data shape: {df_weather.shape}\")\n",
    "print(f\"\\nUnique countries: {df_weather['country'].nunique()}\")\n",
    "print(\"\\nSample country names:\")\n",
    "print(df_weather['country'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The country names appear consistent in this dataset, but let's check for any anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential country name issues\n",
    "\n",
    "def analyze_string_column(series, column_name):\n",
    "    \"\"\"\n",
    "    Analyze a string column for potential quality issues.\n",
    "    \"\"\"\n",
    "    print(f\"Analysis of '{column_name}':\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Convert to string\n",
    "    str_series = series.astype(str)\n",
    "    \n",
    "    # Check for leading/trailing whitespace\n",
    "    has_leading_space = (str_series != str_series.str.lstrip()).sum()\n",
    "    has_trailing_space = (str_series != str_series.str.rstrip()).sum()\n",
    "    print(f\"  Values with leading whitespace: {has_leading_space}\")\n",
    "    print(f\"  Values with trailing whitespace: {has_trailing_space}\")\n",
    "    \n",
    "    # Check for multiple internal spaces\n",
    "    has_multiple_spaces = str_series.str.contains(r'\\s{2,}', regex=True).sum()\n",
    "    print(f\"  Values with multiple internal spaces: {has_multiple_spaces}\")\n",
    "    \n",
    "    # Check case distribution\n",
    "    all_upper = (str_series == str_series.str.upper()).sum()\n",
    "    all_lower = (str_series == str_series.str.lower()).sum()\n",
    "    title_case = (str_series == str_series.str.title()).sum()\n",
    "    print(f\"  All uppercase: {all_upper}\")\n",
    "    print(f\"  All lowercase: {all_lower}\")\n",
    "    print(f\"  Title case: {title_case}\")\n",
    "    \n",
    "    # Check for special characters\n",
    "    has_special = str_series.str.contains(r'[^a-zA-Z0-9\\s\\-\\']', regex=True).sum()\n",
    "    print(f\"  Values with special characters: {has_special}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "analyze_string_column(df_weather['country'], 'country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis function checks for common string quality issues. This helps identify what cleaning is needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Generation for Unique Identifiers\n",
    "\n",
    "### Why Keys Matter\n",
    "\n",
    "Every record should have a unique identifier for:\n",
    "- **Deduplication**: Identifying duplicate records\n",
    "- **Joins**: Linking related tables\n",
    "- **Updates**: Knowing which record to modify\n",
    "- **Tracking**: Following data lineage\n",
    "\n",
    "### Types of Keys\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      KEY TYPES                                  │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  NATURAL KEYS              SURROGATE KEYS                       │\n",
    "│  ├── Exist in the data     ├── Generated/synthetic             │\n",
    "│  ├── Have business meaning ├── No business meaning             │\n",
    "│  ├── May change            ├── Never change                    │\n",
    "│  └── e.g., email, SSN      └── e.g., auto-increment, UUID      │\n",
    "│                                                                 │\n",
    "│  COMPOSITE KEYS            HASH KEYS                            │\n",
    "│  ├── Multiple columns      ├── Hash of column values           │\n",
    "│  ├── Combined uniqueness   ├── Deterministic                   │\n",
    "│  └── e.g., (date, city)    └── e.g., MD5(date+city+temp)       │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_row_hash(df, columns, hash_name='row_hash'):\n",
    "    \"\"\"\n",
    "    Generate a hash-based unique key from multiple columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to add hash to\n",
    "    columns : list\n",
    "        Column names to include in hash\n",
    "    hash_name : str\n",
    "        Name for the new hash column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame with new hash column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Concatenate column values into a string\n",
    "    combined = result[columns].astype(str).agg('|'.join, axis=1)\n",
    "    \n",
    "    # Generate MD5 hash for each row\n",
    "    result[hash_name] = combined.apply(\n",
    "        lambda x: hashlib.md5(x.encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Generate a unique key for air quality records\n",
    "# A record is unique by date + time (one measurement per hour)\n",
    "key_columns = ['Date', 'Time']\n",
    "\n",
    "df_air_clean = generate_row_hash(df_air_clean, key_columns, 'record_id')\n",
    "\n",
    "print(\"Generated record IDs:\")\n",
    "print(df_air_clean[['Date', 'Time', 'record_id']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_row_hash` function creates a deterministic unique identifier by:\n",
    "\n",
    "1. **Concatenating column values**: Joins selected columns with a delimiter\n",
    "2. **Hashing the result**: MD5 produces a 32-character hex string\n",
    "\n",
    "Benefits of hash-based keys:\n",
    "- **Deterministic**: Same input always produces same hash\n",
    "- **Compact**: Fixed length regardless of input size\n",
    "- **Collision-resistant**: Different inputs produce different hashes (with high probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create a composite key for weather data\n",
    "# Using country + location + timestamp\n",
    "\n",
    "def create_composite_key(df, columns, key_name='composite_key', separator='_'):\n",
    "    \"\"\"\n",
    "    Create a human-readable composite key from multiple columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to add key to\n",
    "    columns : list\n",
    "        Column names to combine\n",
    "    key_name : str\n",
    "        Name for new key column\n",
    "    separator : str\n",
    "        Character to separate values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame with new key column\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Clean and combine values\n",
    "    parts = []\n",
    "    for col in columns:\n",
    "        # Convert to string and clean\n",
    "        clean_col = result[col].astype(str).str.lower()\n",
    "        clean_col = clean_col.str.replace(r'[^a-z0-9]', '', regex=True)\n",
    "        parts.append(clean_col)\n",
    "    \n",
    "    # Join with separator\n",
    "    result[key_name] = parts[0]\n",
    "    for part in parts[1:]:\n",
    "        result[key_name] = result[key_name] + separator + part\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Create composite key for weather data\n",
    "df_weather_keyed = create_composite_key(\n",
    "    df_weather.head(1000),  # Sample for demo\n",
    "    ['country', 'location_name', 'last_updated'],\n",
    "    'weather_key'\n",
    ")\n",
    "\n",
    "print(\"Composite keys:\")\n",
    "print(df_weather_keyed[['country', 'location_name', 'last_updated', 'weather_key']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composite keys are human-readable but can be long. They're useful when you need to inspect the key to understand which record it represents.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deduplication Strategies\n",
    "\n",
    "### Types of Duplicates\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    DUPLICATE TYPES                              │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  EXACT DUPLICATES           FUZZY DUPLICATES                    │\n",
    "│  ├── All columns match      ├── Similar but not identical      │\n",
    "│  ├── Easy to detect         ├── Require similarity metrics     │\n",
    "│  └── Drop one copy          └── Need rules for merging         │\n",
    "│                                                                 │\n",
    "│  Example:                   Example:                            │\n",
    "│  \"John Smith, 123 Main\"     \"John Smith, 123 Main St\"          │\n",
    "│  \"John Smith, 123 Main\"     \"J. Smith, 123 Main Street\"        │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Deduplication Approaches\n",
    "\n",
    "1. **Exact match**: Use `drop_duplicates()` on all or selected columns\n",
    "2. **Key-based**: Deduplicate on generated unique key\n",
    "3. **Fuzzy matching**: Use similarity algorithms (Levenshtein, Jaro-Winkler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in air quality data\n",
    "\n",
    "print(\"Duplicate Analysis for Air Quality Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check exact duplicates (all columns)\n",
    "exact_dups = df_air_clean.duplicated().sum()\n",
    "print(f\"\\n1. Exact duplicates (all columns): {exact_dups}\")\n",
    "\n",
    "# Check duplicates on key columns\n",
    "key_dups = df_air_clean.duplicated(subset=['Date', 'Time']).sum()\n",
    "print(f\"2. Duplicate timestamps (Date + Time): {key_dups}\")\n",
    "\n",
    "# If duplicates exist, show them\n",
    "if key_dups > 0:\n",
    "    print(\"\\nSample duplicate timestamps:\")\n",
    "    dup_mask = df_air_clean.duplicated(subset=['Date', 'Time'], keep=False)\n",
    "    print(df_air_clean[dup_mask].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check shows whether our data has duplicate records. The `duplicated()` function marks rows that are duplicates of previous rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_dataframe(df, subset=None, keep='first', report=True):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows with detailed reporting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to deduplicate\n",
    "    subset : list\n",
    "        Column names to check for duplicates (None = all columns)\n",
    "    keep : str\n",
    "        'first' keeps first occurrence, 'last' keeps last, False drops all\n",
    "    report : bool\n",
    "        Print deduplication report\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Deduplicated DataFrame\n",
    "    \"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Find duplicates\n",
    "    dup_mask = df.duplicated(subset=subset, keep=False)\n",
    "    dup_count = dup_mask.sum()\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df_clean = df.drop_duplicates(subset=subset, keep=keep)\n",
    "    final_count = len(df_clean)\n",
    "    \n",
    "    if report:\n",
    "        print(\"Deduplication Report:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Original rows:     {original_count:,}\")\n",
    "        print(f\"  Duplicate rows:    {dup_count:,}\")\n",
    "        print(f\"  Rows removed:      {original_count - final_count:,}\")\n",
    "        print(f\"  Final rows:        {final_count:,}\")\n",
    "        print(f\"  Duplicate rate:    {(original_count - final_count) / original_count * 100:.2f}%\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Deduplicate air quality data\n",
    "df_air_deduped = deduplicate_dataframe(\n",
    "    df_air_clean,\n",
    "    subset=['Date', 'Time'],  # Unique by timestamp\n",
    "    keep='first'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deduplicate_dataframe` function provides a comprehensive report showing exactly how many duplicates were found and removed. The `keep='first'` parameter retains the first occurrence when duplicates are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with intentional duplicates to demonstrate\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': [1, 2, 2, 3, 4, 4, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Bob', 'Charlie', 'David', 'David', 'David', 'Eve'],\n",
    "    'value': [100, 200, 200, 300, 400, 401, 400, 500]  # Note: David has different values\n",
    "})\n",
    "\n",
    "print(\"Original data with duplicates:\")\n",
    "print(sample_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Strategy 1: Keep first occurrence\")\n",
    "print(\"=\"*50)\n",
    "result1 = sample_data.drop_duplicates(subset=['id'], keep='first')\n",
    "print(result1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Strategy 2: Keep last occurrence\")\n",
    "print(\"=\"*50)\n",
    "result2 = sample_data.drop_duplicates(subset=['id'], keep='last')\n",
    "print(result2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Strategy 3: Drop all duplicates\")\n",
    "print(\"=\"*50)\n",
    "result3 = sample_data.drop_duplicates(subset=['id'], keep=False)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three strategies show different outcomes:\n",
    "\n",
    "1. **keep='first'**: Keeps row 2 (Bob, 200), row 5 (David, 400)\n",
    "2. **keep='last'**: Keeps row 3 (Bob, 200), row 7 (David, 400)\n",
    "3. **keep=False**: Removes ALL duplicates, keeping only unique rows (Alice, Charlie, Eve)\n",
    "\n",
    "The choice depends on your data semantics. If duplicates have different values (like David), you need a rule for which value to keep.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building Reusable Cleaning Functions\n",
    "\n",
    "### The Data Cleaning Pipeline Pattern\n",
    "\n",
    "Rather than writing one-off cleaning code, build a reusable pipeline:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                  DATA CLEANING PIPELINE                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  Raw Data ───► Clean Types ───► Standardize ───► Dedupe        │\n",
    "│       │              │              │              │            │\n",
    "│       ▼              ▼              ▼              ▼            │\n",
    "│  [Audit Log]   [Audit Log]   [Audit Log]    [Audit Log]        │\n",
    "│                                                                 │\n",
    "│                        │                                        │\n",
    "│                        ▼                                        │\n",
    "│                  Clean Data + Quality Report                    │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"\n",
    "    A reusable data cleaning pipeline.\n",
    "    \n",
    "    This class encapsulates common cleaning operations with\n",
    "    automatic logging and quality reporting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, name='dataset'):\n",
    "        \"\"\"\n",
    "        Initialize the cleaner with a DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Raw data to clean\n",
    "        name : str\n",
    "            Name for logging purposes\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.name = name\n",
    "        self.log = []  # Track all operations\n",
    "        self.original_shape = df.shape\n",
    "        \n",
    "        self._log(f\"Initialized with {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    def _log(self, message):\n",
    "        \"\"\"Add entry to operation log.\"\"\"\n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'operation': message,\n",
    "            'rows': len(self.df),\n",
    "            'columns': len(self.df.columns)\n",
    "        }\n",
    "        self.log.append(entry)\n",
    "        print(f\"  [{entry['rows']:,} rows] {message}\")\n",
    "    \n",
    "    def drop_columns(self, columns):\n",
    "        \"\"\"Drop specified columns.\"\"\"\n",
    "        existing = [c for c in columns if c in self.df.columns]\n",
    "        self.df = self.df.drop(columns=existing)\n",
    "        self._log(f\"Dropped columns: {existing}\")\n",
    "        return self\n",
    "    \n",
    "    def drop_empty_columns(self):\n",
    "        \"\"\"Drop columns that are entirely null.\"\"\"\n",
    "        empty_cols = [c for c in self.df.columns if self.df[c].isna().all()]\n",
    "        if empty_cols:\n",
    "            self.df = self.df.drop(columns=empty_cols)\n",
    "            self._log(f\"Dropped {len(empty_cols)} empty columns\")\n",
    "        return self\n",
    "    \n",
    "    def convert_european_decimals(self, columns):\n",
    "        \"\"\"Convert European decimal format to numeric.\"\"\"\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].astype(str).str.replace(',', '.', regex=False)\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        self._log(f\"Converted European decimals in {len(columns)} columns\")\n",
    "        return self\n",
    "    \n",
    "    def parse_dates(self, column, format=None):\n",
    "        \"\"\"Parse date column.\"\"\"\n",
    "        self.df[column] = pd.to_datetime(self.df[column], format=format, errors='coerce')\n",
    "        self._log(f\"Parsed dates in '{column}'\")\n",
    "        return self\n",
    "    \n",
    "    def normalize_strings(self, columns, case='lower'):\n",
    "        \"\"\"Normalize string columns.\"\"\"\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].astype(str).str.strip()\n",
    "                self.df[col] = self.df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                if case == 'lower':\n",
    "                    self.df[col] = self.df[col].str.lower()\n",
    "                elif case == 'upper':\n",
    "                    self.df[col] = self.df[col].str.upper()\n",
    "                elif case == 'title':\n",
    "                    self.df[col] = self.df[col].str.title()\n",
    "        self._log(f\"Normalized {len(columns)} string columns\")\n",
    "        return self\n",
    "    \n",
    "    def replace_values(self, column, mapping):\n",
    "        \"\"\"Replace values according to mapping.\"\"\"\n",
    "        self.df[column] = self.df[column].replace(mapping)\n",
    "        self._log(f\"Replaced values in '{column}'\")\n",
    "        return self\n",
    "    \n",
    "    def replace_missing_codes(self, columns, codes, replacement=np.nan):\n",
    "        \"\"\"Replace coded missing values with NaN.\"\"\"\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].replace(codes, replacement)\n",
    "        self._log(f\"Replaced missing codes {codes} in {len(columns)} columns\")\n",
    "        return self\n",
    "    \n",
    "    def add_hash_key(self, columns, key_name='row_hash'):\n",
    "        \"\"\"Generate hash key from columns.\"\"\"\n",
    "        combined = self.df[columns].astype(str).agg('|'.join, axis=1)\n",
    "        self.df[key_name] = combined.apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        self._log(f\"Added hash key '{key_name}' from {columns}\")\n",
    "        return self\n",
    "    \n",
    "    def deduplicate(self, subset=None, keep='first'):\n",
    "        \"\"\"Remove duplicate rows.\"\"\"\n",
    "        before = len(self.df)\n",
    "        self.df = self.df.drop_duplicates(subset=subset, keep=keep)\n",
    "        removed = before - len(self.df)\n",
    "        self._log(f\"Removed {removed:,} duplicate rows\")\n",
    "        return self\n",
    "    \n",
    "    def get_result(self):\n",
    "        \"\"\"Return the cleaned DataFrame.\"\"\"\n",
    "        return self.df\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate cleaning report.\"\"\"\n",
    "        report = {\n",
    "            'dataset': self.name,\n",
    "            'original_shape': self.original_shape,\n",
    "            'final_shape': self.df.shape,\n",
    "            'rows_removed': self.original_shape[0] - len(self.df),\n",
    "            'columns_removed': self.original_shape[1] - len(self.df.columns),\n",
    "            'operations': len(self.log),\n",
    "            'log': self.log\n",
    "        }\n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataCleaner` class provides a fluent interface for data cleaning. Key design features:\n",
    "\n",
    "1. **Method chaining**: Each method returns `self`, allowing `cleaner.drop_columns().convert().dedupe()`\n",
    "2. **Automatic logging**: Every operation is tracked with timestamp and row count\n",
    "3. **Non-destructive**: Original data is copied, never modified\n",
    "4. **Reporting**: Get a summary of all operations performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the DataCleaner on our air quality data\n",
    "\n",
    "# Reload raw data for a fresh start\n",
    "df_air_raw = pd.read_csv(air_quality_path, sep=';', encoding='utf-8')\n",
    "\n",
    "# Define columns with European decimals\n",
    "numeric_cols = ['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', \n",
    "                'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)',\n",
    "                'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
    "\n",
    "print(\"Cleaning Air Quality Dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create cleaner and run pipeline\n",
    "cleaner = DataCleaner(df_air_raw, 'AirQualityUCI')\n",
    "\n",
    "df_cleaned = (\n",
    "    cleaner\n",
    "    .drop_empty_columns()\n",
    "    .convert_european_decimals(numeric_cols)\n",
    "    .parse_dates('Date', format='%d/%m/%Y')\n",
    "    .replace_missing_codes(numeric_cols, [-200, -200.0])\n",
    "    .add_hash_key(['Date', 'Time'], 'record_id')\n",
    "    .deduplicate(subset=['Date', 'Time'])\n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Cleaning Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline ran all cleaning steps in sequence. The log shows what happened at each step with row counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the cleaning report\n",
    "report = cleaner.get_report()\n",
    "\n",
    "print(\"\\nCleaning Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {report['dataset']}\")\n",
    "print(f\"Original shape: {report['original_shape']}\")\n",
    "print(f\"Final shape: {report['final_shape']}\")\n",
    "print(f\"Rows removed: {report['rows_removed']}\")\n",
    "print(f\"Columns removed: {report['columns_removed']}\")\n",
    "print(f\"Total operations: {report['operations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report provides a summary that can be saved for audit purposes. This documentation is essential for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the cleaned data\n",
    "print(\"Cleaned Data Quality Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. Shape: {df_cleaned.shape}\")\n",
    "\n",
    "print(f\"\\n2. Data Types:\")\n",
    "for col in df_cleaned.columns[:10]:  # First 10 columns\n",
    "    print(f\"   {col}: {df_cleaned[col].dtype}\")\n",
    "\n",
    "print(f\"\\n3. Missing Values:\")\n",
    "missing = df_cleaned.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = count / len(df_cleaned) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values!\")\n",
    "\n",
    "print(f\"\\n4. Sample Data:\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality check confirms our cleaning was successful. Note that replacing -200 with NaN creates \"missing values\" - this is correct because -200 was actually a code for missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. Understanding Dirty Data\n",
    "- Data quality issues are pervasive and costly\n",
    "- Always inspect raw data before processing\n",
    "- Common issues: wrong types, inconsistent formats, duplicates, missing values\n",
    "\n",
    "### 2. Type Standardization\n",
    "- Explicit type conversion prevents silent errors\n",
    "- Use `pd.to_numeric()` with `errors='coerce'` for safe conversion\n",
    "- Define schemas explicitly for consistency\n",
    "\n",
    "### 3. European Number Formats\n",
    "- Many European countries use comma as decimal separator\n",
    "- Replace comma with period before numeric conversion\n",
    "- Look for semicolon delimiters as a sign of European CSV\n",
    "\n",
    "### 4. Date Parsing\n",
    "- Date formats vary by region and system\n",
    "- Specify format explicitly when possible\n",
    "- Combine Date + Time for full timestamp\n",
    "\n",
    "### 5. String Normalization\n",
    "- Strip whitespace, normalize case\n",
    "- Use regex for pattern-based cleaning\n",
    "- Build mapping tables for known variations\n",
    "\n",
    "### 6. Key Generation\n",
    "- Every record needs a unique identifier\n",
    "- Hash keys are deterministic and compact\n",
    "- Composite keys are human-readable\n",
    "\n",
    "### 7. Deduplication\n",
    "- Check for duplicates before analysis\n",
    "- Choose strategy based on data semantics (first/last/drop all)\n",
    "- Document deduplication decisions\n",
    "\n",
    "### 8. Reusable Pipelines\n",
    "- Encapsulate cleaning logic in classes/functions\n",
    "- Log all operations for reproducibility\n",
    "- Generate reports for audit trails\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Books\n",
    "- McKinney, W. (2022). *Python for Data Analysis* (3rd ed.). O'Reilly Media.\n",
    "  - Chapter 7: Data Cleaning and Preparation\n",
    "- McCallum, Q. E. (2012). *Bad Data Handbook*. O'Reilly Media.\n",
    "- Wickham, H. (2014). \"Tidy Data.\" *Journal of Statistical Software*, 59(10), 1-23.\n",
    "\n",
    "### Documentation\n",
    "- [pandas String Methods](https://pandas.pydata.org/docs/user_guide/text.html)\n",
    "- [pandas Working with Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [pandas Duplicated](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)\n",
    "\n",
    "### Best Practices\n",
    "- [Google Data Preparation Guidelines](https://developers.google.com/machine-learning/data-prep)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Clean the Food Data\n",
    "Load `FOOD-DATA-GROUP2.csv` and clean it using the `DataCleaner` class. This dataset has unnamed columns and potential duplicates.\n",
    "\n",
    "### Exercise 2: Date Format Detective\n",
    "Write a function that examines a date column and determines whether it's in US (MM/DD/YYYY) or European (DD/MM/YYYY) format based on value ranges.\n",
    "\n",
    "### Exercise 3: Fuzzy Deduplication\n",
    "Research the `fuzzywuzzy` library and implement a function that finds \"near-duplicate\" rows based on string similarity.\n",
    "\n",
    "### Exercise 4: Data Quality Dashboard\n",
    "Create a function that generates a comprehensive data quality report including:\n",
    "- Missing value rates per column\n",
    "- Duplicate rates\n",
    "- Type analysis\n",
    "- Value range checks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Class: Data Cleaning II\n",
    "\n",
    "In Lecture 10, we will cover:\n",
    "- Outlier detection methods (IQR, Z-score, domain-based)\n",
    "- Advanced missing value handling\n",
    "- Validation rules and constraints\n",
    "- Referential integrity checks\n",
    "- Data quality scoring frameworks\n",
    "\n",
    "We will continue building on the cleaning pipeline we developed today.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Lecture 9*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
