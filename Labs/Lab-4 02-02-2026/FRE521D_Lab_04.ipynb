{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lab 4: ETL Tools - Building an Automated Pipeline\n",
    "\n",
    "**Program:** UBC Master of Food and Resource Economics  \n",
    "**Instructor:** Asif Ahmed Neloy\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #FFF3CD; border-left: 4px solid #E6A23C; padding: 15px; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #856404;\">Submission Deadline</h3>\n",
    "    <p style=\"margin-bottom: 0; font-size: 1.2em;\"><strong>End of Day: Tuesday, Feb 03, 2026</strong></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "In this lab, you will build an automated ETL pipeline with professional features. You will:\n",
    "\n",
    "1. Create a configuration class for pipeline settings\n",
    "2. Implement proper logging instead of print statements\n",
    "3. Build functions for Extract, Transform, and Load phases\n",
    "4. Add retry logic with exponential backoff\n",
    "5. Generate a pipeline execution report\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the `GlobalWeatherRepository.csv` dataset, which contains weather observations from locations around the world.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sqlite3\n",
    "import functools\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 1: Configuration Class (15 points)\n",
    "\n",
    "## Task: Create a Pipeline Configuration Class\n",
    "\n",
    "As covered in Lecture 8, configuration should be centralized in a class that can read from environment variables with sensible defaults.\n",
    "\n",
    "**Complete the class below** to include:\n",
    "- `source_path`: Path to the CSV file (default: `'../../Datasets/GlobalWeatherRepository.csv'`)\n",
    "- `database_path`: Path to SQLite database (default: `'weather_pipeline.db'`)\n",
    "- `table_name`: Name of destination table (default: `'weather_data'`)\n",
    "- `chunk_size`: Rows per chunk for loading (default: `10000`)\n",
    "- `log_file`: Path to log file (default: `'pipeline.log'`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for the ETL pipeline.\n",
    "    Reads from environment variables with sensible defaults.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # ============================================\n",
    "        # YOUR CODE HERE: Define the 5 configuration attributes\n",
    "        # Use os.getenv() to allow environment variable overrides\n",
    "        # ============================================\n",
    "        \n",
    "        # 1. source_path - path to source CSV file\n",
    "        self.source_path = None  # Replace with os.getenv(...)\n",
    "        \n",
    "        # 2. database_path - path to SQLite database\n",
    "        self.database_path = None  # Replace with os.getenv(...)\n",
    "        \n",
    "        # 3. table_name - destination table name\n",
    "        self.table_name = None  # Replace with os.getenv(...)\n",
    "        \n",
    "        # 4. chunk_size - rows per chunk (convert to int!)\n",
    "        self.chunk_size = None  # Replace with int(os.getenv(...))\n",
    "        \n",
    "        # 5. log_file - path to log file\n",
    "        self.log_file = None  # Replace with os.getenv(...)\n",
    "        \n",
    "        # ============================================\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Return configuration as dictionary.\"\"\"\n",
    "        return {\n",
    "            'source_path': self.source_path,\n",
    "            'database_path': self.database_path,\n",
    "            'table_name': self.table_name,\n",
    "            'chunk_size': self.chunk_size,\n",
    "            'log_file': self.log_file\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your configuration class\n",
    "config = PipelineConfig()\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Verify all values are set\n",
    "assert config.source_path is not None, \"source_path not set!\"\n",
    "assert config.database_path is not None, \"database_path not set!\"\n",
    "assert config.table_name is not None, \"table_name not set!\"\n",
    "assert config.chunk_size is not None, \"chunk_size not set!\"\n",
    "assert config.log_file is not None, \"log_file not set!\"\n",
    "print(\"\\nAll configuration values set correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 2: Logging Setup (15 points)\n",
    "\n",
    "## Task: Create a Logging Configuration Function\n",
    "\n",
    "Production pipelines use logging instead of print statements. As covered in Lecture 8, we need both file and console handlers.\n",
    "\n",
    "**Complete the function below** to:\n",
    "1. Create a logger with the name 'etl_pipeline'\n",
    "2. Add a file handler that writes to the specified log file\n",
    "3. Add a console handler that shows INFO level and above\n",
    "4. Use this format: `'%(asctime)s | %(levelname)-8s | %(message)s'`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file, level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configure logging for the ETL pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_file : str\n",
    "        Path to the log file\n",
    "    level : int\n",
    "        Logging level (default: logging.INFO)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    logging.Logger : Configured logger\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    # Step 1: Create logger\n",
    "    logger = logging.getLogger('etl_pipeline')\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Clear existing handlers (important in Jupyter)\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Step 2: Create formatter\n",
    "    # Use format: '%(asctime)s | %(levelname)-8s | %(message)s'\n",
    "    formatter = None  # YOUR CODE HERE\n",
    "    \n",
    "    # Step 3: Create and add file handler\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 4: Create and add console handler\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your logging function\n",
    "logger = setup_logging(config.log_file)\n",
    "\n",
    "logger.info(\"Testing INFO message\")\n",
    "logger.warning(\"Testing WARNING message\")\n",
    "logger.error(\"Testing ERROR message\")\n",
    "\n",
    "print(f\"\\nLog file created: {config.log_file}\")\n",
    "print(\"Check the log file to verify messages were written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 3: Retry Decorator (20 points)\n",
    "\n",
    "## Task: Implement Retry Logic with Exponential Backoff\n",
    "\n",
    "Network operations can fail temporarily. We need retry logic that waits longer between each attempt (exponential backoff).\n",
    "\n",
    "**Complete the decorator below** to:\n",
    "1. Attempt the function up to `max_retries + 1` times\n",
    "2. If it fails, wait `base_delay * (2 ** attempt)` seconds before retrying\n",
    "3. Log each retry attempt using the logger\n",
    "4. If all attempts fail, raise the last exception\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_backoff(max_retries=3, base_delay=1):\n",
    "    \"\"\"\n",
    "    Decorator that retries a function with exponential backoff.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_retries : int\n",
    "        Maximum number of retry attempts\n",
    "    base_delay : int\n",
    "        Base delay in seconds (doubles each retry)\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            # ============================================\n",
    "            # YOUR CODE HERE\n",
    "            # ============================================\n",
    "            \n",
    "            # Loop through attempts (0 to max_retries)\n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    # Try to execute the function\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt < max_retries:\n",
    "                        # Calculate delay: base_delay * (2 ** attempt)\n",
    "                        delay = None  # YOUR CODE HERE\n",
    "                        \n",
    "                        # Log the retry (use logger.warning)\n",
    "                        # YOUR CODE HERE\n",
    "                        \n",
    "                        # Wait before retrying\n",
    "                        time.sleep(delay)\n",
    "                    else:\n",
    "                        # All attempts failed - log error\n",
    "                        # YOUR CODE HERE\n",
    "                        pass\n",
    "            \n",
    "            # ============================================\n",
    "            \n",
    "            # Raise the last exception\n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retry decorator\n",
    "\n",
    "# This function fails 2 times then succeeds\n",
    "fail_count = 0\n",
    "\n",
    "@retry_with_backoff(max_retries=3, base_delay=1)\n",
    "def unreliable_function():\n",
    "    global fail_count\n",
    "    fail_count += 1\n",
    "    if fail_count <= 2:\n",
    "        raise ConnectionError(f\"Simulated failure #{fail_count}\")\n",
    "    return \"Success!\"\n",
    "\n",
    "print(\"Testing retry decorator...\")\n",
    "result = unreliable_function()\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Total attempts: {fail_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 4: ETL Functions (30 points)\n",
    "\n",
    "## Task: Implement Extract, Transform, and Load Functions\n",
    "\n",
    "Now we'll build the core ETL functions. Each function should:\n",
    "- Log its progress using the logger\n",
    "- Return useful information about what it did\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(source_path):\n",
    "    \"\"\"\n",
    "    Extract data from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    source_path : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Extracted data\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    # 1. Log that extraction is starting\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # 2. Check if file exists, raise FileNotFoundError if not\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # 3. Read the CSV file\n",
    "    df = None  # YOUR CODE HERE\n",
    "    \n",
    "    # 4. Log how many rows were extracted\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Transform the weather data.\n",
    "    \n",
    "    Transformations:\n",
    "    1. Parse last_updated to datetime\n",
    "    2. Add temperature category column\n",
    "    3. Add ETL metadata columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Raw extracted data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Transformed data\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    logger.info(\"Starting transformation...\")\n",
    "    \n",
    "    # Make a copy\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Parse last_updated to datetime\n",
    "    df_clean['last_updated_dt'] = pd.to_datetime(df_clean['last_updated'], errors='coerce')\n",
    "    \n",
    "    # 2. Add temperature category based on temperature_celsius\n",
    "    # Categories: 'Cold' (<10), 'Mild' (10-25), 'Hot' (>25)\n",
    "    # Use pd.cut() - YOUR CODE HERE\n",
    "    df_clean['temp_category'] = None  # YOUR CODE HERE\n",
    "    \n",
    "    # 3. Add ETL metadata\n",
    "    # _etl_timestamp: current datetime as ISO string\n",
    "    # _etl_source: the source file path from config\n",
    "    df_clean['_etl_timestamp'] = None  # YOUR CODE HERE\n",
    "    df_clean['_etl_source'] = None  # YOUR CODE HERE\n",
    "    \n",
    "    # 4. Log transformation complete\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, database_path, table_name, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Load data into SQLite database.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Data to load\n",
    "    database_path : str\n",
    "        Path to SQLite database\n",
    "    table_name : str\n",
    "        Destination table name\n",
    "    chunk_size : int\n",
    "        Rows per chunk\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of rows loaded\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    # 1. Log that loading is starting\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # 2. Create database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    \n",
    "    # 3. Load data using to_sql with if_exists='replace'\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # 4. Verify the load by counting rows\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    \n",
    "    # 5. Close connection\n",
    "    conn.close()\n",
    "    \n",
    "    # 6. Log how many rows were loaded\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ETL functions\n",
    "print(\"Testing ETL Functions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract\n",
    "df_raw = extract(config.source_path)\n",
    "print(f\"\\nExtracted shape: {df_raw.shape}\")\n",
    "\n",
    "# Transform\n",
    "df_transformed = transform(df_raw)\n",
    "print(f\"Transformed shape: {df_transformed.shape}\")\n",
    "print(f\"New columns: {[c for c in df_transformed.columns if c not in df_raw.columns]}\")\n",
    "\n",
    "# Load\n",
    "rows_loaded = load(df_transformed, config.database_path, config.table_name)\n",
    "print(f\"Rows loaded: {rows_loaded:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 5: Pipeline Execution Report (20 points)\n",
    "\n",
    "## Task: Create a Function to Generate Execution Reports\n",
    "\n",
    "After running the pipeline, we need a report that summarizes what happened.\n",
    "\n",
    "**Complete the function below** to generate a report including:\n",
    "- Pipeline name and execution timestamp\n",
    "- Duration in seconds\n",
    "- Rows extracted, transformed, loaded\n",
    "- Status (success/failed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(pipeline_name, start_time, end_time, metrics, status='success', error=None):\n",
    "    \"\"\"\n",
    "    Generate a pipeline execution report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline_name : str\n",
    "        Name of the pipeline\n",
    "    start_time : datetime\n",
    "        When the pipeline started\n",
    "    end_time : datetime\n",
    "        When the pipeline ended\n",
    "    metrics : dict\n",
    "        Dictionary of metrics (rows_extracted, rows_transformed, rows_loaded)\n",
    "    status : str\n",
    "        'success' or 'failed'\n",
    "    error : str\n",
    "        Error message if failed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Formatted report\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # Build the report string\n",
    "    report = f\"\"\"\n",
    "{'='*60}\n",
    "PIPELINE EXECUTION REPORT\n",
    "{'='*60}\n",
    "\n",
    "Pipeline: {pipeline_name}\n",
    "Status: {status.upper()}\n",
    "\n",
    "Timing:\n",
    "  Start: {start_time.isoformat()}\n",
    "  End: {end_time.isoformat()}\n",
    "  Duration: {duration:.2f} seconds\n",
    "\n",
    "Metrics:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add metrics to report\n",
    "    for key, value in metrics.items():\n",
    "        report += f\"  {key}: {value:,}\\n\"  # YOUR CODE: format the value\n",
    "    \n",
    "    # Add error if present\n",
    "    if error:\n",
    "        report += f\"\\nError: {error}\\n\"\n",
    "    \n",
    "    report += f\"\\n{'='*60}\\n\"\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline and generate report\n",
    "\n",
    "print(\"Running Complete ETL Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "metrics = {}\n",
    "\n",
    "try:\n",
    "    # Extract\n",
    "    df_raw = extract(config.source_path)\n",
    "    metrics['rows_extracted'] = len(df_raw)\n",
    "    \n",
    "    # Transform\n",
    "    df_transformed = transform(df_raw)\n",
    "    metrics['rows_transformed'] = len(df_transformed)\n",
    "    \n",
    "    # Load\n",
    "    rows_loaded = load(df_transformed, config.database_path, config.table_name)\n",
    "    metrics['rows_loaded'] = rows_loaded\n",
    "    \n",
    "    status = 'success'\n",
    "    error = None\n",
    "    \n",
    "except Exception as e:\n",
    "    status = 'failed'\n",
    "    error = str(e)\n",
    "\n",
    "# Record end time\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Generate and print report\n",
    "report = generate_report('weather_etl', start_time, end_time, metrics, status, error)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verify Your Work\n",
    "\n",
    "Run this cell to verify the pipeline worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"Pipeline Verification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check database was created\n",
    "print(f\"\\n1. Database exists: {os.path.exists(config.database_path)}\")\n",
    "\n",
    "# Check table contents\n",
    "conn = sqlite3.connect(config.database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM {config.table_name}\")\n",
    "row_count = cursor.fetchone()[0]\n",
    "print(f\"2. Rows in table: {row_count:,}\")\n",
    "\n",
    "# Check for new columns\n",
    "sample = pd.read_sql_query(f\"SELECT * FROM {config.table_name} LIMIT 5\", conn)\n",
    "print(f\"3. temp_category column exists: {'temp_category' in sample.columns}\")\n",
    "print(f\"4. _etl_timestamp column exists: {'_etl_timestamp' in sample.columns}\")\n",
    "\n",
    "# Check log file\n",
    "print(f\"5. Log file exists: {os.path.exists(config.log_file)}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Question 1**: PipelineConfig class has all 5 attributes with defaults\n",
    "- [ ] **Question 2**: Logging function creates both file and console handlers\n",
    "- [ ] **Question 3**: Retry decorator implements exponential backoff correctly\n",
    "- [ ] **Question 4**: All three ETL functions work and log their progress\n",
    "- [ ] **Question 5**: Report function generates complete execution summary\n",
    "- [ ] All verification checks pass\n",
    "\n",
    "### How to Submit\n",
    "\n",
    "1. Save this notebook\n",
    "2. Submit via Canvas (Lab 4 Submission) by **end of day Tuesday, Feb 03, 2026**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
