{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lab 3: Python Wrangling - Tidy Data, Types, and Validation\n",
    "\n",
    "**Program:** UBC Master of Food and Resource Economics  \n",
    "**Instructor:** Asif Ahmed Neloy\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #FFF3CD; border-left: 4px solid #E6A23C; padding: 15px; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #856404;\">Submission Deadline</h3>\n",
    "    <p style=\"margin-bottom: 0; font-size: 1.2em;\"><strong>Wednesday, January 21, 2026 - 11:59 PM (End of Day)</strong></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "In this lab, you will apply Python wrangling techniques to the climate-agriculture data from Assignment 1. You will:\n",
    "\n",
    "1. **Read** data from your MySQL database tables\n",
    "2. **Check and convert** data types appropriately\n",
    "3. **Reshape** data from wide to long format using `pd.melt()`\n",
    "4. **Analyze and handle** missing data\n",
    "5. **Validate** data quality with range, null, and uniqueness checks\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Assignment 1 completed (tables created in your MySQL database)\n",
    "- Docker container running with MySQL\n",
    "- Conda environment activated\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "# Database connection configuration\n",
    "# Update these values if your setup is different\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"mfre521d_user\",\n",
    "    \"password\": \"mfre521d_user_pw\",\n",
    "    \"database\": \"mfre521d\",\n",
    "}\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Create and return a database connection.\"\"\"\n",
    "    return mysql.connector.connect(**DB_CONFIG)\n",
    "\n",
    "def read_table(query):\n",
    "    \"\"\"Execute a SQL query and return results as DataFrame.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    print(\"Connected to MySQL successfully!\")\n",
    "    conn.close()\n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure your Docker container is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 1: Read Data and Check Types\n",
    "\n",
    "## Task: Load Data from A1 Tables and Inspect Data Types\n",
    "\n",
    "As discussed in Lecture 6, **always check `df.dtypes` after loading data**. Different sources may encode the same information differently.\n",
    "\n",
    "### Part A: Read the tables from your database\n",
    "\n",
    "The solution for reading from the database is provided below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries: 0 rows\n",
      "Crop Production: 83740 rows\n",
      "Temperature Anomalies: 1137 rows\n",
      "\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION PROVIDED: Read data from A1 tables\n",
    "\n",
    "# Read countries table\n",
    "df_countries = read_table(\"SELECT * FROM dim_country\")\n",
    "print(f\"Countries: {len(df_countries)} rows\")\n",
    "\n",
    "# Read crop_production table\n",
    "df_crops = read_table(\"SELECT * FROM Crop_2\")\n",
    "print(f\"Crop Production: {len(df_crops)} rows\")\n",
    "\n",
    "# Read temperature_anomalies table\n",
    "df_temp = read_table(\"SELECT * FROM Temp_2\")\n",
    "print(f\"Temperature Anomalies: {len(df_temp)} rows\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year\n",
       "0  2001\n",
       "1  1993\n",
       "2  1995\n",
       "3  2018\n",
       "4  2013"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1) clean year to numeric\n",
    "df_crops['year'] = pd.to_numeric(df_crops['year'], errors='coerce')\n",
    "\n",
    "# 2) quick check\n",
    "df_crops['year'].dtype\n",
    "df_crops[['year']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                  object\n",
       "iso3_code                object\n",
       "region                   object\n",
       "income_group             object\n",
       "year                      int64\n",
       "crop                     object\n",
       "area_harvested_ha         int64\n",
       "production_tonnes       float64\n",
       "yield_kg_ha             float64\n",
       "fertilizer_use_kg_ha    float64\n",
       "irrigation_pct          float64\n",
       "notes                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = [\n",
    "    'area_harvested_ha',\n",
    "    'production_tonnes',\n",
    "    'yield_kg_ha',\n",
    "    'fertilizer_use_kg_ha',\n",
    "    'irrigation_pct'\n",
    "]\n",
    "\n",
    "for c in num_cols:\n",
    "    df_crops[c] = (\n",
    "        df_crops[c].astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\"%\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "    df_crops[c] = pd.to_numeric(df_crops[c], errors='coerce')\n",
    "\n",
    "df_crops.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COUNTRIES TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>iso3_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>region</th>\n",
       "      <th>income_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [country_id, iso3_code, country_name, region, income_group]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View sample of each table\n",
    "print(\"=\" * 60)\n",
    "print(\"COUNTRIES TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROP PRODUCTION TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>iso3_code</th>\n",
       "      <th>region</th>\n",
       "      <th>income_group</th>\n",
       "      <th>year</th>\n",
       "      <th>crop</th>\n",
       "      <th>area_harvested_ha</th>\n",
       "      <th>production_tonnes</th>\n",
       "      <th>yield_kg_ha</th>\n",
       "      <th>fertilizer_use_kg_ha</th>\n",
       "      <th>irrigation_pct</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>Upper middle income</td>\n",
       "      <td>2001</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>3751494</td>\n",
       "      <td>12036421.75</td>\n",
       "      <td>3208.43</td>\n",
       "      <td>100.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>NPL</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>Low income</td>\n",
       "      <td>1993</td>\n",
       "      <td>Maize</td>\n",
       "      <td>2112762</td>\n",
       "      <td>11377270.55</td>\n",
       "      <td>5385.02</td>\n",
       "      <td>19.14</td>\n",
       "      <td>9.8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>KOR</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>High income</td>\n",
       "      <td>1995</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>1650777</td>\n",
       "      <td>7474101.16</td>\n",
       "      <td>4527.63</td>\n",
       "      <td>193.84</td>\n",
       "      <td>56.6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>North America</td>\n",
       "      <td>High income</td>\n",
       "      <td>2018</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>4782989</td>\n",
       "      <td>32397951.41</td>\n",
       "      <td>6773.58</td>\n",
       "      <td>205.12</td>\n",
       "      <td>62.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>JPN</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>High income</td>\n",
       "      <td>2013</td>\n",
       "      <td>Rice</td>\n",
       "      <td>5434696</td>\n",
       "      <td>58322509.35</td>\n",
       "      <td>10731.51</td>\n",
       "      <td>211.64</td>\n",
       "      <td>61.4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country iso3_code         region         income_group  year  \\\n",
       "0          China       CHN      East Asia  Upper middle income  2001   \n",
       "1          Nepal       NPL     South Asia           Low income  1993   \n",
       "2    South Korea       KOR      East Asia          High income  1995   \n",
       "3  United States       USA  North America          High income  2018   \n",
       "4          Japan       JPN      East Asia          High income  2013   \n",
       "\n",
       "       crop  area_harvested_ha  production_tonnes  yield_kg_ha  \\\n",
       "0  Soybeans            3751494        12036421.75      3208.43   \n",
       "1     Maize            2112762        11377270.55      5385.02   \n",
       "2  Soybeans            1650777         7474101.16      4527.63   \n",
       "3     Wheat            4782989        32397951.41      6773.58   \n",
       "4      Rice            5434696        58322509.35     10731.51   \n",
       "\n",
       "   fertilizer_use_kg_ha  irrigation_pct notes  \n",
       "0                100.90             NaN  None  \n",
       "1                 19.14             9.8  None  \n",
       "2                193.84            56.6  None  \n",
       "3                205.12            62.5  None  \n",
       "4                211.64            61.4  None  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CROP PRODUCTION TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPERATURE ANOMALIES TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>annual_anomaly_c</th>\n",
       "      <th>jan</th>\n",
       "      <th>feb</th>\n",
       "      <th>mar</th>\n",
       "      <th>apr</th>\n",
       "      <th>may</th>\n",
       "      <th>jun</th>\n",
       "      <th>jul</th>\n",
       "      <th>aug</th>\n",
       "      <th>sep</th>\n",
       "      <th>oct</th>\n",
       "      <th>nov</th>\n",
       "      <th>dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>1993</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    country  year  annual_anomaly_c   jan   feb   mar   apr  \\\n",
       "0  United States of America  1990              0.07 -0.02   NaN   NaN -0.09   \n",
       "1  United States of America  1991              0.20  0.36  0.40  0.60  0.74   \n",
       "2  United States of America  1992              0.54  0.46  0.76  0.85  0.68   \n",
       "3  United States of America  1993              0.43  0.28  0.22  0.74  0.60   \n",
       "4  United States of America  1994              0.87  0.75  0.86  0.65  1.14   \n",
       "\n",
       "    may   jun   jul   aug   sep   oct   nov   dec  \n",
       "0 -0.11  0.44 -0.44  0.30 -0.08 -0.03 -0.31 -0.39  \n",
       "1  0.22  0.34  0.22  0.70   NaN -0.44 -0.50 -0.22  \n",
       "2  0.60  0.98  0.52  0.92  0.53  0.01  0.09  0.07  \n",
       "3   NaN  1.30  0.81  0.35 -0.30  0.04  0.55 -0.13  \n",
       "4  0.45  1.71  1.27  1.04  0.01  0.62  1.05  0.88  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEMPERATURE ANOMALIES TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Check and Document Data Types\n",
    "\n",
    "**YOUR TASK:** \n",
    "1. Use `.dtypes` to check the data types of each DataFrame\n",
    "2. Use `.info()` to get a summary including non-null counts\n",
    "3. Answer the questions in the markdown cell below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country                  object\n",
      "iso3_code                object\n",
      "region                   object\n",
      "income_group             object\n",
      "year                      int64\n",
      "crop                     object\n",
      "area_harvested_ha         int64\n",
      "production_tonnes       float64\n",
      "yield_kg_ha             float64\n",
      "fertilizer_use_kg_ha    float64\n",
      "irrigation_pct          float64\n",
      "notes                    object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83740 entries, 0 to 83739\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   country               83740 non-null  object \n",
      " 1   iso3_code             83740 non-null  object \n",
      " 2   region                83740 non-null  object \n",
      " 3   income_group          83740 non-null  object \n",
      " 4   year                  83740 non-null  int64  \n",
      " 5   crop                  83740 non-null  object \n",
      " 6   area_harvested_ha     83740 non-null  int64  \n",
      " 7   production_tonnes     81240 non-null  float64\n",
      " 8   yield_kg_ha           81240 non-null  float64\n",
      " 9   fertilizer_use_kg_ha  81240 non-null  float64\n",
      " 10  irrigation_pct        81240 non-null  float64\n",
      " 11  notes                 4180 non-null   object \n",
      "dtypes: float64(4), int64(2), object(6)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check data types for df_crops\n",
    "# ============================================\n",
    "\n",
    "# Print the data types\n",
    "\n",
    "print(df_crops.dtypes)\n",
    "# Print info (includes non-null counts)\n",
    "\n",
    "df_crops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country              object\n",
      "year                  int64\n",
      "annual_anomaly_c    float64\n",
      "jan                 float64\n",
      "feb                 float64\n",
      "mar                 float64\n",
      "apr                 float64\n",
      "may                 float64\n",
      "jun                 float64\n",
      "jul                 float64\n",
      "aug                 float64\n",
      "sep                 float64\n",
      "oct                 float64\n",
      "nov                 float64\n",
      "dec                 float64\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1137 entries, 0 to 1136\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   country           1137 non-null   object \n",
      " 1   year              1137 non-null   int64  \n",
      " 2   annual_anomaly_c  1137 non-null   float64\n",
      " 3   jan               1114 non-null   float64\n",
      " 4   feb               1114 non-null   float64\n",
      " 5   mar               1114 non-null   float64\n",
      " 6   apr               1115 non-null   float64\n",
      " 7   may               1115 non-null   float64\n",
      " 8   jun               1114 non-null   float64\n",
      " 9   jul               1114 non-null   float64\n",
      " 10  aug               1114 non-null   float64\n",
      " 11  sep               1114 non-null   float64\n",
      " 12  oct               1114 non-null   float64\n",
      " 13  nov               1114 non-null   float64\n",
      " 14  dec               1114 non-null   float64\n",
      "dtypes: float64(13), int64(1), object(1)\n",
      "memory usage: 133.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check data types for df_temp\n",
    "# ============================================\n",
    "\n",
    "# Print the data types\n",
    "\n",
    "print(df_temp.dtypes)\n",
    "# Print info\n",
    "df_temp.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crops['year'].dtype\n",
    "df_crops['yield_kg_ha'].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Answer These Questions (write your answers below)\n",
    "\n",
    "1. What is the data type of the `year` column in `df_crops`?\n",
    "\n",
    "   **Your answer:*int64*\n",
    "\n",
    "2. What is the data type of the `yield_kg_ha` column? Why is it this type?\n",
    "\n",
    "   **Your answer:*float64* \n",
    "\n",
    "3. How many monthly temperature columns are there in `df_temp`? List them.\n",
    "\n",
    "   **Your answer:*since crop_2 those fields are stored as varchar in the data, so it can be read as object* \n",
    "\n",
    "4. Which column in `df_crops` has the most NULL values? How many?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 2: Reshape Data - Wide to Long (25 points)\n",
    "\n",
    "## Task: Convert Monthly Temperature Data from Wide to Long Format\n",
    "\n",
    "The `temperature_anomalies` table has **monthly data stored in wide format** (columns: jan, feb, mar, ..., dec). This is a classic case where we need to reshape the data to make it **tidy**.\n",
    "\n",
    "### Tidy Data Principle\n",
    "- Each **variable** should have its own **column**\n",
    "- Each **observation** should have its own **row**\n",
    "- Each **value** should have its own **cell**\n",
    "\n",
    "### Current Structure (Wide - NOT Tidy)\n",
    "```\n",
    "country_id | year | annual_anomaly_c | jan  | feb  | mar  | ... | dec\n",
    "-----------+------+------------------+------+------+------+-----+-----\n",
    "    1      | 2020 |      1.5         | 1.2  | 1.8  | 1.4  | ... | 1.6\n",
    "```\n",
    "\n",
    "### Target Structure (Long - Tidy)\n",
    "```\n",
    "country_id | year | month | monthly_anomaly_c\n",
    "-----------+------+-------+------------------\n",
    "    1      | 2020 |  jan  |       1.2\n",
    "    1      | 2020 |  feb  |       1.8\n",
    "    1      | 2020 |  mar  |       1.4\n",
    "    ...    | ...  |  ...  |       ...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current temperature table structure:\n",
      "Shape: (1137, 15)\n",
      "\n",
      "Columns: ['country', 'year', 'annual_anomaly_c', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n"
     ]
    }
   ],
   "source": [
    "# First, let's look at the current structure\n",
    "print(\"Current temperature table structure:\")\n",
    "print(f\"Shape: {df_temp.shape}\")\n",
    "print(f\"\\nColumns: {df_temp.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Use pd.melt() to Reshape the Data\n",
    "\n",
    "**YOUR TASK:** Complete the code below to:\n",
    "1. Use `pd.melt()` to convert monthly columns to rows\n",
    "2. Keep `country_id`, `year`, and `annual_anomaly_c` as identifier columns\n",
    "3. Melt the month columns (jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec)\n",
    "4. Name the new columns: `month` and `monthly_anomaly_c`\n",
    "\n",
    "**Hint:** Use the syntax:\n",
    "```python\n",
    "pd.melt(df, \n",
    "        id_vars=['cols', 'to', 'keep'], \n",
    "        value_vars=['cols', 'to', 'unpivot'],\n",
    "        var_name='new_col_name', \n",
    "        value_name='value_col_name')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m month_columns = [\u001b[33m'\u001b[39m\u001b[33mjan\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfeb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmar\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mapr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmay\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjun\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      7\u001b[39m                  \u001b[33m'\u001b[39m\u001b[33mjul\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maug\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msep\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33moct\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnov\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdec\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Use pd.melt() to reshape from wide to long\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df_temp_long = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43m___\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# columns to keep as identifiers\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43m___\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# columns to unpivot\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m___\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# name for the new 'month' column\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m___\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# name for the new value column\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fre521d/lib/python3.11/site-packages/pandas/core/reshape/melt.py:53\u001b[39m, in \u001b[36mmelt\u001b[39m\u001b[34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mmelt\u001b[39m\u001b[33m\"\u001b[39m] % {\u001b[33m\"\u001b[39m\u001b[33mcaller\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpd.melt(df, \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataFrame.melt\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmelt\u001b[39m(\n\u001b[32m     45\u001b[39m     frame: DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     ignore_index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     52\u001b[39m ) -> DataFrame:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalue_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     55\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalue_name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) cannot match an element in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthe DataFrame columns.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         )\n\u001b[32m     58\u001b[39m     id_vars = ensure_list_vars(id_vars, \u001b[33m\"\u001b[39m\u001b[33mid_vars\u001b[39m\u001b[33m\"\u001b[39m, frame.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/fre521d/lib/python3.11/site-packages/pandas/core/indexes/base.py:5370\u001b[39m, in \u001b[36mIndex.__contains__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m   5336\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5337\u001b[39m \u001b[33;03m    Return a boolean indicating whether the provided key is in the index.\u001b[39;00m\n\u001b[32m   5338\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5368\u001b[39m \u001b[33;03m    False\u001b[39;00m\n\u001b[32m   5369\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5370\u001b[39m     \u001b[38;5;28mhash\u001b[39m(key)\n\u001b[32m   5371\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   5372\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'DataFrame'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Reshape temperature data\n",
    "# ============================================\n",
    "\n",
    "# Define the month columns to unpivot\n",
    "month_columns = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', \n",
    "                 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# Use pd.melt() to reshape from wide to long\n",
    "df_temp_long = pd.melt(\n",
    "    df_temp,\n",
    "    id_vars=___,  # columns to keep as identifiers\n",
    "    value_vars=___,  # columns to unpivot\n",
    "    var_name=___,  # name for the new 'month' column\n",
    "    value_name=___  # name for the new value column\n",
    ")\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the reshape\n",
    "print(\"Reshaped temperature data:\")\n",
    "print(f\"Original shape: {df_temp.shape}\")\n",
    "print(f\"New shape: {df_temp_long.shape}\")\n",
    "print(f\"\\nExpected rows: {len(df_temp)} Ã— 12 months = {len(df_temp) * 12}\")\n",
    "print(f\"Actual rows: {len(df_temp_long)}\")\n",
    "\n",
    "print(\"\\nSample of reshaped data:\")\n",
    "df_temp_long.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Add Month Number for Sorting\n",
    "\n",
    "**YOUR TASK:** Create a `month_num` column that converts month names to numbers (jan=1, feb=2, ..., dec=12)\n",
    "\n",
    "**Hint:** Create a dictionary mapping and use `.map()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Add month number column\n",
    "# ============================================\n",
    "\n",
    "# Create a mapping dictionary\n",
    "month_to_num = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "    'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "    'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "# Add the month_num column using .map()\n",
    "df_temp_long['month_num'] = ___\n",
    "\n",
    "# Sort by country_id, year, month_num\n",
    "df_temp_long = df_temp_long.sort_values(['country_id', 'year', 'month_num'])\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the month numbers\n",
    "print(\"Temperature data with month numbers:\")\n",
    "df_temp_long[['country_id', 'year', 'month', 'month_num', 'monthly_anomaly_c']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Answer These Questions\n",
    "\n",
    "1. Why is the long format considered \"tidy\" for this data?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. What is the formula to calculate the expected number of rows after melting?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. When would you want to convert BACK from long to wide format?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 3: Missing Data Analysis (25 points)\n",
    "\n",
    "## Task: Analyze and Handle Missing Data\n",
    "\n",
    "As discussed in Lecture 6, understanding **why** data is missing helps decide how to handle it:\n",
    "\n",
    "| Type | Description | Strategy |\n",
    "|------|-------------|----------|\n",
    "| **MCAR** | Missing Completely At Random | Drop or impute |\n",
    "| **MAR** | Missing At Random (depends on other variables) | Impute with care |\n",
    "| **MNAR** | Missing Not At Random | Cannot ignore |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Calculate Missing Data Statistics\n",
    "\n",
    "**YOUR TASK:** Create a function that calculates missing data statistics for any DataFrame.\n",
    "\n",
    "The function should return a DataFrame with:\n",
    "- Column name\n",
    "- Total count\n",
    "- Missing count\n",
    "- Missing percentage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Create missing data function\n",
    "# ============================================\n",
    "\n",
    "def missing_data_report(df):\n",
    "    \"\"\"\n",
    "    Generate a missing data report for a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns: column, total, missing, missing_pct\n",
    "    \"\"\"\n",
    "    # Calculate total rows\n",
    "    total = len(df)\n",
    "    \n",
    "    # Calculate missing for each column\n",
    "    # Hint: use df.isnull().sum() for missing count\n",
    "    \n",
    "    report = pd.DataFrame({\n",
    "        'column': ___,\n",
    "        'total': ___,\n",
    "        'missing': ___,\n",
    "        'missing_pct': ___\n",
    "    })\n",
    "    \n",
    "    # Round percentage to 2 decimal places\n",
    "    report['missing_pct'] = report['missing_pct'].round(2)\n",
    "    \n",
    "    # Sort by missing_pct descending\n",
    "    report = report.sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on crop_production\n",
    "print(\"Missing Data Report: Crop Production\")\n",
    "print(\"=\" * 50)\n",
    "missing_crops = missing_data_report(df_crops)\n",
    "missing_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on temperature (long format)\n",
    "print(\"Missing Data Report: Temperature Anomalies (Long)\")\n",
    "print(\"=\" * 50)\n",
    "missing_temp = missing_data_report(df_temp_long)\n",
    "missing_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Analyze Missing Patterns\n",
    "\n",
    "**YOUR TASK:** Investigate which countries/years have the most missing temperature data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Find countries with most missing monthly data\n",
    "# ============================================\n",
    "\n",
    "# Group by country_id and count missing values\n",
    "# Hint: use .isnull() and .sum() after groupby\n",
    "\n",
    "missing_by_country = df_temp_long.groupby('country_id')['monthly_anomaly_c'].apply(\n",
    "    lambda x: ___  # count null values\n",
    ").reset_index()\n",
    "\n",
    "missing_by_country.columns = ['country_id', 'missing_months']\n",
    "\n",
    "# Sort by missing count descending and show top 10\n",
    "missing_by_country = missing_by_country.sort_values('missing_months', ascending=False)\n",
    "print(\"Countries with most missing monthly temperature data:\")\n",
    "missing_by_country.head(10)\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Handle Missing Data\n",
    "\n",
    "**YOUR TASK:** For the `df_crops` DataFrame, handle missing `yield_kg_ha` values using **group-based imputation** (fill with the mean yield for each crop type).\n",
    "\n",
    "This is appropriate when we believe the missing mechanism is **MAR** - missing values depend on the crop type.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Impute missing yield with crop-specific mean\n",
    "# ============================================\n",
    "\n",
    "# Make a copy to avoid modifying original\n",
    "df_crops_imputed = df_crops.copy()\n",
    "\n",
    "# Count missing before\n",
    "missing_before = df_crops_imputed['yield_kg_ha'].isnull().sum()\n",
    "print(f\"Missing yield values before: {missing_before}\")\n",
    "\n",
    "# Calculate mean yield for each crop\n",
    "crop_mean_yield = df_crops_imputed.groupby('crop')['yield_kg_ha'].transform('mean')\n",
    "\n",
    "# Fill missing values with crop-specific mean\n",
    "# Hint: use fillna()\n",
    "df_crops_imputed['yield_kg_ha'] = df_crops_imputed['yield_kg_ha'].fillna(___)\n",
    "\n",
    "# Count missing after\n",
    "missing_after = df_crops_imputed['yield_kg_ha'].isnull().sum()\n",
    "print(f\"Missing yield values after: {missing_after}\")\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a flag column to track which values were imputed\n",
    "df_crops_imputed['yield_imputed'] = df_crops['yield_kg_ha'].isnull()\n",
    "\n",
    "print(f\"Rows with imputed yield: {df_crops_imputed['yield_imputed'].sum()}\")\n",
    "\n",
    "# Show some imputed rows\n",
    "print(\"\\nSample of imputed rows:\")\n",
    "df_crops_imputed[df_crops_imputed['yield_imputed']][['country_id', 'year', 'crop', 'yield_kg_ha', 'yield_imputed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Answer These Questions\n",
    "\n",
    "1. What percentage of `yield_kg_ha` values were missing in the original data?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. Why did we use crop-specific mean instead of overall mean for imputation?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. Why is it important to create a flag column (`yield_imputed`) to track imputed values?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 4: Data Validation (25 points)\n",
    "\n",
    "## Task: Implement Validation Checks\n",
    "\n",
    "As discussed in Lecture 6, validation catches problems early:\n",
    "\n",
    "| Type | Check | Example |\n",
    "|------|-------|----------|\n",
    "| **Range** | Values within bounds | Year between 1900-2100 |\n",
    "| **Null** | Required fields present | country_id not null |\n",
    "| **Type** | Correct data type | Year is integer |\n",
    "| **Uniqueness** | No duplicates | Unique country-year-crop combo |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Range Validation\n",
    "\n",
    "**YOUR TASK:** Check that values are within expected ranges:\n",
    "- `year`: between 1900 and 2100\n",
    "- `yield_kg_ha`: between 0 and 50000 (reasonable crop yield)\n",
    "- `irrigation_pct`: between 0 and 100\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Implement range validation\n",
    "# ============================================\n",
    "\n",
    "def validate_range(df, column, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Check if values in a column are within the specified range.\n",
    "    \n",
    "    Returns: DataFrame with rows that FAIL validation\n",
    "    \"\"\"\n",
    "    # Find rows where value is outside range (excluding nulls)\n",
    "    # Hint: use (df[column] < min_val) | (df[column] > max_val)\n",
    "    \n",
    "    mask = ___\n",
    "    \n",
    "    invalid_rows = df[mask]\n",
    "    return invalid_rows\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test range validations\n",
    "print(\"Range Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check year range\n",
    "invalid_year = validate_range(df_crops, 'year', 1900, 2100)\n",
    "print(f\"Invalid year values (outside 1900-2100): {len(invalid_year)}\")\n",
    "\n",
    "# Check yield range\n",
    "invalid_yield = validate_range(df_crops, 'yield_kg_ha', 0, 50000)\n",
    "print(f\"Invalid yield values (outside 0-50000): {len(invalid_yield)}\")\n",
    "\n",
    "# Check irrigation range\n",
    "invalid_irrigation = validate_range(df_crops, 'irrigation_pct', 0, 100)\n",
    "print(f\"Invalid irrigation values (outside 0-100): {len(invalid_irrigation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Null Validation\n",
    "\n",
    "**YOUR TASK:** Check that required fields are not null:\n",
    "- `country_id`: Required\n",
    "- `year`: Required\n",
    "- `crop`: Required\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Implement null validation\n",
    "# ============================================\n",
    "\n",
    "def validate_not_null(df, columns):\n",
    "    \"\"\"\n",
    "    Check that specified columns have no null values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "    columns : list of column names\n",
    "    \n",
    "    Returns: dict with column name and count of nulls\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for col in columns:\n",
    "        # Count null values in each column\n",
    "        null_count = ___\n",
    "        results[col] = null_count\n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test null validation\n",
    "required_columns = ['country_id', 'year', 'crop']\n",
    "null_results = validate_not_null(df_crops, required_columns)\n",
    "\n",
    "print(\"Null Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "for col, count in null_results.items():\n",
    "    status = \"PASS\" if count == 0 else \"FAIL\"\n",
    "    print(f\"{col}: {count} nulls [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Uniqueness Validation\n",
    "\n",
    "**YOUR TASK:** Check that there are no duplicate records for the same country-year-crop combination.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check for duplicates\n",
    "# ============================================\n",
    "\n",
    "def validate_unique(df, columns):\n",
    "    \"\"\"\n",
    "    Check that combinations of columns are unique.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "    columns : list of columns that should be unique together\n",
    "    \n",
    "    Returns: DataFrame with duplicate rows\n",
    "    \"\"\"\n",
    "    # Find duplicates\n",
    "    # Hint: use df.duplicated(subset=columns, keep=False)\n",
    "    \n",
    "    duplicates = df[___]\n",
    "    return duplicates\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test uniqueness validation\n",
    "unique_cols = ['country_id', 'year', 'crop']\n",
    "duplicates = validate_unique(df_crops, unique_cols)\n",
    "\n",
    "print(\"Uniqueness Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Duplicate country-year-crop combinations: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nSample duplicates:\")\n",
    "    print(duplicates[unique_cols + ['production_tonnes']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Create a Complete Validation Report\n",
    "\n",
    "**YOUR TASK:** Combine all validations into a summary report.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Create validation summary\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA VALIDATION SUMMARY - CROP PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(df_crops)}\")\n",
    "print(f\"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# 1. Range checks\n",
    "print(\"1. RANGE CHECKS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Year (1900-2100): {len(validate_range(df_crops, 'year', 1900, 2100))} failures\")\n",
    "print(f\"   Yield (0-50000): {len(validate_range(df_crops, 'yield_kg_ha', 0, 50000))} failures\")\n",
    "print(f\"   Irrigation (0-100): {len(validate_range(df_crops, 'irrigation_pct', 0, 100))} failures\")\n",
    "print()\n",
    "\n",
    "# 2. Null checks\n",
    "print(\"2. NULL CHECKS (Required Fields)\")\n",
    "print(\"-\" * 40)\n",
    "for col, count in validate_not_null(df_crops, ['country_id', 'year', 'crop']).items():\n",
    "    status = \"PASS\" if count == 0 else f\"FAIL ({count} nulls)\"\n",
    "    print(f\"   {col}: {status}\")\n",
    "print()\n",
    "\n",
    "# 3. Uniqueness check\n",
    "print(\"3. UNIQUENESS CHECK\")\n",
    "print(\"-\" * 40)\n",
    "dup_count = len(validate_unique(df_crops, ['country_id', 'year', 'crop']))\n",
    "status = \"PASS\" if dup_count == 0 else f\"FAIL ({dup_count} duplicates)\"\n",
    "print(f\"   country_id + year + crop: {status}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Answer These Questions\n",
    "\n",
    "1. Why is range validation important for data quality?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. What would you do if you found duplicate country-year-crop records?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. List one additional validation check that would be useful for this dataset.\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Question 1**: Checked data types and answered all questions\n",
    "- [ ] **Question 2**: Successfully reshaped temperature data from wide to long\n",
    "- [ ] **Question 3**: Created missing data report and implemented imputation\n",
    "- [ ] **Question 4**: Implemented all three validation checks (range, null, unique)\n",
    "- [ ] All markdown questions have been answered\n",
    "\n",
    "### How to Submit\n",
    "\n",
    "1. Save this notebook\n",
    "2. Export as PDF or HTML\n",
    "3. Submit via Canvas by **Wednesday, January 21, 2026 at 11:59 PM**\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "| Question | Points | Description |\n",
    "|----------|--------|-------------|\n",
    "| Q1 | 15 | Data type inspection and questions |\n",
    "| Q2 | 25 | Reshape (melt) implementation |\n",
    "| Q3 | 25 | Missing data analysis and imputation |\n",
    "| Q4 | 25 | Validation checks implementation |\n",
    "| **Style** | 10 | Code quality, comments, formatting |\n",
    "| **Total** | **100** | |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fre521d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
